---
layout: post
title: Regression Project
---

My data was focused on soccer players and their stats from the FIFA 2019 database. As a result, it required extensive cleaning, as it contained columns with images (useless to data analysis). Many values, like height, weight, wage, and value of player also contained additional words like lbs (signifying pounds) or M (signifying millions) or K (signifying thousands) which the computer registered as strings rather than number, so I had to separate the integers from these words. After plotting several of the variables against each other, I found I had to log the value, wage, and international reputation of a player to even the distribution of these values. At this point I realized the value and wage of a player included several 0s, and I decided to drop these rows, as I had plenty of data and assumed that these 0s were used as placeholders rather than actually meaning the player made or was worth nothing. I had several categorical variables (Nationality, Club, Preferred Foot, Work Rate, and Position) which I changed into numerical variables using pd.get_dummies and amassed in one massive one_hot.

At this point, I had over 800 columns, making sns.pairplotting impossible. I decided to separate the columns by the level of correlation of the variable with the value of a player (my dependent variable) (calculated by R^2), and only sns.pairplotted those variables with an R^2 greater than 0.35. Wage was correlated most highly with value of a player (R^2 = 0.85), followed by international reputation and overall ability and potential of a player. Not surprisingly, Real Madrid and FC Barcelona were correlated pretty highly with the value of a player (0.18 and 0.17). The most negatively correlated variables were a medium/medium work rate (-0.14) and a Chinese or English nationality (-0.055 and -0.053).

By logging Value, Wage, and International Reputation, I increased the R^2 and adjusted R^2 values from 0.8002395343179898 and 0.7997474583752058 to 0.9708498748293883 and 0.9707813400523414. I checked out the rest of my most highly correlated variables in sns.pairplot to see if any of them would be better suited to a quadratic or higher degree relationship with value, but all had a minimized mean square error between the train and test groups at degree 1. As a result, I did a ridge regression using all the variables as they were at this point, and found the ideal alpha for my final regression was 0.1, with an average mean squared error of 0.057. This had an equation of Player value  = -4.66246513e-01age + 1.51421202e+00overall - 2.56309848e-02*potential + (additional 800+ variables). I also created a simplified model using just the highest correlated values, which was still highly accurate (R^2 = 0.94299 and adjusted R^2 = 0.94296), which had the equation: Player value  = 0.9427363overall + 0.4202552potential + 0.12572617special - 0.03355369reactions + 0.03613vision - 0.04762959composure - 0.04355927log of reputation + 0.11289603log of wage.

In the context of this dataset, low variance means that my ridge regression has a much more general slope that can fit both the training and test groups of players' stats data, while low bias means that my model, despite this, is still able to reasonably predict the value of a player.
