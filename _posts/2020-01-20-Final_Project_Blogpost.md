---
layout: post
title: Final Project
---
Speech recognition is a staple of technology nowadays, with Siri a constant on iPhones and tech companies competing for domination in the home audio assistant market. Deep learning especially has made speech recognition more popular, allowing for large improvements in accuracy when used in the real world. Consequently, most research now leans towards the use of recurrent neural networks, as these are able to construct a transcript of an audio recording based on each letter spoken in a sentence, using the previous letters and a massive database of sentences and text to make educated guesses about what should come next (for a more intensive linguistic explanation of this process, see [here](https://cmusphinx.github.io/wiki/tutorialconcepts/)). Though all this may seem fairly simple, the process actually requires an extensive amount of data, with millions of speakers going over every possible variation of letter and word pronunciation. As a result, I decided to scale my project down by utilizing the basic techniques of speech recognition but instead comparing full sentences using a model, K Nearest Neighbors, which is typically used for audio classification projects.

I worked with the DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus, produced by MIT, Stanford Research Institute, and Texas Instruments. The corpus contains two sentences, labeled SA1 and SA2, that were created with the purpose of showcasing the variety in pronunciation across US dialect regions and were spoken by all 630 speakers in the study. 450 sentences, labelled SX, focused instead on maximizing the phones spoken, with each sentence spoken by seven different people. The last category, SI sentences, numbered 1890, with each only spoken once and being collected from other texts to increase the complexity of the phonetic compositions of the sentences in the corpus. The makeup of the speakers is shown here, and their full data can be accessed [here](https://www.kaggle.com/mfekadu/darpa-timit-acousticphonetic-continuous-speech):
![](/images/dialect.png)

I chose pyAudioAnalysis, an open-source Python library, to analyze the .wav files that make up the DARPA TIMIT corpus. The download instructions, which are just two steps, can be found [here](https://github.com/tyiannak/pyAudioAnalysis/wiki/2.-General). The first official coding step, then, was the creation of an audio analysis function so that I would be able to extract the Mel Frequency Cepstral Coefficients (MFCCs) of each recording in multiple datasets without having to copy and customize the same lines of code. The function I made, audio_analysis(audio_file, dataset) takes in the name of the audio_file to be analyzed, as well as whether it is part of the test or train dataset. The function first uses the read_audio_file function from audioBasicIO to convert the waveform of the .wav recording into numerical data. This process is known as sampling, and it takes the height of the sound wave at discreet times, pretty accurately describing the wave (thanks to [this](https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a) article for teaching me the basics of audio analysis as well!):
![](https://miro.medium.com/max/3200/1*dICZCcmEm_EWWx0yA6B3Cw.gif)
Then, my function uses ShortTermFeatures.feature_extraction() to find the features at each frame size of 50 msecs with a frame step of 25 msecs. I decided to use only the MFCCs in my analysis, as I already had a fairly large dataset and the MFCCs contain the essential information from each frame (go [here](https://opensource.com/article/19/9/audio-processing-machine-learning-python) to read more about the mathematical basis behind MFCCs). My function then averaged each MFCC, appending this data to a list. I also appended the name of the file to another list, as this would tell me to which sentence the data corresponded.

I had to use several for loops, os.listdir(), and .endswith(".wav") to run through all the data contained in the DARPA TIMIT corpus, as the data were organized in folders of dialect region and speaker and contained additional non-recording files that supplemented the recordings with linguistic information. Once the data were inserted into train and test datasets, following the division the researchers had already chosen, I created a new function, called analyze(tr, te), that took in a training and testing set and performed the necessary scaling before implementing GridSearchCV with a K Nearest Neighbors model between 1 and 30 neighbors. The function returned R^2, the model, the X_training array, the X_testing array, the y_training array, and the y_testing array in that order. However, in running the function on the whole dataset, I ran into a problemâ€”the SI sentences only had one speaker, which was not sufficient for the model (see the error below):
![](/images/error.png)
The model resulted in an R^2 value of 0.176, with 20 neighbors, and so I decided to drop all the sentences that were only spoken once. In doing so, however, I committed another error. I correctly created a dictionary with the key being the sentence and the value being the number of times it appeared in the training set, but instead of creating new datasets with sentences spoken greater than one time, I coded for datasets not spoken only once in the training dataframe. Thus, as the SI sentences were only spoken once, the code accepted the SI sentences in the testing dataset, as they were not part of the list of unacceptable sentences that was made off the training set. Though at first I was initially confused as to why my supposedly improved dataset was resulting in a similar R^2 value of 0.175, I soon figured out the issue, and resolved it, instead coding for only sentences not spoken only once to be part of the acceptable sentences list and basing the datasets directly off of this list. 

The model, now running on a dataset with sentences spoken by multiple speakers, resulted in an R^2 value of 0.881, remarkably high for the variability present in pronunciation. This R^2 value was only slightly lower than using only the two sentences spoken by every speaker in the project, SA1 and SA2 (R^2 0.902), and so I decided to continue my analysis with the more diverse dataset. I found that, even though SA2 and SA1 were the sentences most prevalent in the dataset, they were also the ones most confused, with most errors stemming from the model misclassifying SA2 sentences as SA1. The only other misclassifications were SA2 as SX169, and SA1 as SX423 and SX123.  The sentences themselves, however, seemed completely different:
![](/images/sentences.png)

I decided to create a function called comparison(s1, s2) that takes in two sentences, averages each of the MFCCs, finds the standard deviation of each of the MFCCs, and prints out each value for comparison, as well as the difference between the averages and the average of these differences. SA2 and SA1 were not very similar, having an average difference of 0.100 between all the MFCCs that was within the range of the standard deviations for each, the highest being 0.115. This was also the case for SA1 and SX169, with an average difference of 0.137, and SA1 and SX123, with an average difference of 0.070. SA1 and SX423 were the only ones very close together, having an average difference of 0.077 that was smaller than most of the standard deviations. Consequently, I was still unsure as to why SA2 was so often confused with SA1.

Furthermore, interestingly enough, the dialect regions with the greatest number of errors were DR2 and DR4, with DR4 doing slightly worse when only SA2 sentence misclassifications were taken into account. As a result, I decided to create a chord diagram that would show me the similarities and differences between six SA1 and SA2 sentences spoken by the same DR4 dialect region speakers. Doing so, however, proved an arduously complex task. The function visualizeFeaturesFolder from audioVisualization was supposed to take in a path to a folder containing all the data and a dimensionality reduction method, either PCA or LDA, and return several files that would allow me to run an interactive chord diagram. Running PCA, however, I received a matrix of only 0s, while running LDA I received the following error:
![](/images/errorLDA.png)
I was forced to dive deep into the code of the visualizeFeaturesFolder() function itself to find a solution. I decided to copy and paste the code into jupyter notebook, to allow me ease of access, typing, and testing. Then, I systematically commented out blocks of code related to the PCA option until I found the single line that had been my bane:
![](/images/chord.png)
All of my values were smaller than 0.5, and yet the threshold assumed for the chord diagram was 0.5, which was why the matrix that resulted was always filled with only 0s. Changing the threshold to 0 resulted in the following diagram, with SA2 sentences in red and SA1 sentences in blue (The diagram can also be viewed by downloading the visualization_Chordial folder found [here](https://github.com/krzalica/Final_Project), navigating to the folder in the terminal, and creating your own python http server by typing `python3 -m http.server 8000`):
![](/images/chorddiagram.png)

The function also resulted in this graph of the reduced values:
![](/images/graph.png)

After using the same troubleshooting method for LDA, I realized that the function visualizeFeaturesFolder had a third positional argument, priorKnowledge, which was originally set to 'none' and which the function used to define the classes when using LDA as its dimensionality reduction method. This was at odds with the official guide itself, which instead stated that with LDA, a supervised model, "the required labels are taken from the subcategories of the input files (if available). These are provided through the respective filenames, using the string `---` as a separator." (See the rest of the guide on data visualization with pyAudioAnalysis [here](https://github.com/tyiannak/pyAudioAnalysis/wiki/6.-Data-visualization)). LDA was taking in each of the file names and assuming they each corresponded to one class because each of the names was different, requiring priorKnowledge to classify them, while PCA instead did what the guide stated LDA would do:
![](/images/names.png) 

I decided to continue using PCA instead of changing to LDA because my data was already set up for the splitting of the file names with `---` used in PCA.

I also created a spectrogram of one speaker from the DR4 dialect region saying both SA1 and SA2 in the terminal using `python audioAnalysis.py fileSpectrogram -i FILENAME`.

SA1:
![](/images/SA1_Spectrogram.png)

SA2:
![](/images/SA2_Spectrogram.png)

Thus it seems that, overall, SA1 and SA2, when spoken by someone from the South Midland dialect region (see diagram below for a closer map of area, with orange being South Midland), are quite similar.
![](https://upload.wikimedia.org/wikipedia/commons/9/99/Midland_American_English_map.jpg)

The full code can be found [here](https://github.com/krzalica/Final_Project/blob/master/Final%20Project.ipynb).