---
layout: post
title: Final_Project_Blogpost
---
Speech recognition is a staple of technology nowadays, with Siri present on every single iPhone made and several other major companies competing for domination over the in-home audio helper. Deep learning especially has made research on the topic more popular, allowing for greater accuracy in the real world. Now, consequently, most research leans towards the use of recurrent neural networks to construct a transcript of the recording based on each letter spoken in a sentence (for a more intensive linguistic explanation of this process, see [here](https://cmusphinx.github.io/wiki/tutorialconcepts/). This model is often paired with a massive database of sentences and text, guiding the phonetic transcription to the actual words being said (the difference between "He wud net go" and the actual spelling "He would not go). Though all this may seem fairly simple, the process actually requires a massive dataset, with millions of speakers going over every possible variation of letter pronunciation. As a result, I decided to scale my project down by using similar basic techniques to speech recognition but instead using a model, K Nearest Neighbors, which is typically used for audio classification projects, to compare full sentences rather than individual letters.

I worked with the DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus, produced by MIT, Stanford Research Institute, and Texas Instruments. The corpus contains two sentences, labelled SA1 and SA2, that were created with the purpose of showcasing the variety in pronunciation across US dialect regions and were spoken by all 630 speakers from across the United States. 450 sentences, labelled SX, focused instead on maximizing the phones spoken, with each sentence spoken by seven different people. The last category, SI sentences, numbered 1890, with each only spoken once and being collected from other texts to increase the complexity of the phonetic compositions of the sentences in the corpus. The researchers had already divided the data into test and train sets. The makeup of the speakers is shown here, and their data can be accessed [here](https://www.kaggle.com/mfekadu/darpa-timit-acousticphonetic-continuous-speech):
![](dialect.png)

I chose pyAudioAnalysis, an open-source Python library, to analyze the .wav files that make up the DARPA TIMIT corpus. The download instructions, which are just two steps, can be found [here](https://github.com/tyiannak/pyAudioAnalysis/wiki/2.-General). The first official coding step, then, was the creation of an audio analysis function so that I would be able to extract the Mel Frequency Cepstral Coefficients (MFCCs) of each recording in multiple datasets without having to copy and customize the same lines of code. The function audio_analysis(audio_file, dataset) takes in the name of the audio_file to be analyzed, as well as whether it is part of the test or train dataset. The function first uses the read_audio_file function from audioBasicIO to convert the waveform of the .wav recording into numerical data. This process is known as sampling, and it takes the height of the sound wave at discreet times, pretty accurately describing the wave (thanks to [this](https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a) article for teaching me the basics of audio analysis as well!):
![](https://miro.medium.com/max/3200/1*dICZCcmEm_EWWx0yA6B3Cw.gif)
Then, it uses ShortTermFeatures.feature_extraction() to find the features at each frame size of 50 msecs with a frame step of 25 mess. I decided to use only the MFCCs in my analysis, as I already had a fairly large dataset and the MFCCs contain the essential information from each frame (go [here](https://opensource.com/article/19/9/audio-processing-machine-learning-python) to read more about the mathematical basis behind MFCCs), and averaged each MFCC, appending this data to a list. I also appended the name of the file to another list, as this would tell me to which sentence the data corresponded.

I had to use several for loops, os.listdir(), and .endswith(".wav") to run through all the data contained in the DARPA TIMIT corpus, as it was organized in folders dialect region and speaker and contained additional non-recording files that supplemented the recordings with linguistic data. Once the data were inserted into train and test datasets, I created a new function, called analyze(tr, te), that took in a training and testing set and performed the necessary scaling before implementing GridSearchCV with a K Nearest Neighbors model between 1 and 30 neighbors. The function returned R^2, the model, the X_training array, the X_testing array, the y_training array, and the y_testing array in that order. However, in running the function on the whole dataset, I ran into a problemâ€”the SI sentences only had one speaker, which was not sufficient for the model (see the error below):
![](error.png)
The model resulted in an R^2 value of 0.176, with 20 neighbors, and so I decided to drop all the sentences that were only spoken once. In doing so, however, I committed another error. I correctly created a dictionary with the key being the sentence and the value being the number of times it appeared in the training set, but instead of creating new datasets with sentences spoken greater than one time, I coded for datasets not spoken only once in the training data frame. Thus, as the SI sentences were only spoken once, the code accepted the SI sentences in the testing dataset, as they were not part of the list of unacceptable sentences that was made off the training set. Though at first I was initially confused as to why my supposedly improved dataset was resulting in similar R^2 of 0.175, I soon figured out the issue, and resolved it. 

The model, running on a dataset with sentences spoken by at least seven speakers, resulted in an R^2 of 0.881, remarkably high for the variability present in pronunciation. This R^2 was only slightly lower than only using the two sentences spoken by every speaker in the project, SA1 and SA2, 0.902, and so I decided to continue my analysis with the more diverse dataset. I found that, even though SA2 and SA1 were the sentences most prevalent in the dataset, they were also the ones most confused, with most errors stemming from the model misclassifying SA2 sentences as SA1. The only other misclassifications were SA2 and SX169, and SA1 as SX423 and SX123.  The sentences themselves, to add further confusion, seemed extremely distinct:
![](sentences.png)

I decided to create a function called comparison(s1,s2) that takes in two sentences, averages each of the MFCCs, finds the standard deviation of each of the MFCCs, and prints out each value for comparison, as well as the difference between the averages and the average of these differences. SA2 and SA1 were strikingly similar, having an average difference of 0.039 between all the MFCCs that was well within the range of all the standard deviations, none of which were less than 0.065. This was also the case for SA1 and SX169, with an average difference of 0.038, and SA1 and SX123, with an average difference of 0.013. SA1 and SX423 were the only ones a little more distinct, having an average difference of 0.065 that was larger than a few standard deviations. Consequently, I was still unsure as to why SA2 was so often confused with SA1.

Furthermore, interestingly enough, the dialect regions with the greatest number of errors were DR2 and DR4, with DR4 doing slightly worse when only SA2 sentence misclassifications were taken into account. As a result, I decided to create a chord diagram that would show me the similarities and differences between six SA1 and SA2 sentences spoken by the same DR4 dialect region people. Doing so, however, proved an arduously complex task. The function visualizeFeaturesFolder from audioVisualization was supposed to take in a path to a folder containing all the data and a dimensionality reduction method, either PCA or LDA. Running PCA, I received a matrix of only 0s, , while running LDA gave me the following error:
![](errorLDA.png)
I was forced to dive deep into the code of the function itself to find a solution. I decided to copy and paste the code into jupyter notebook, to allow me ease of access, typing, and testing. Then, I systematically commented out blocks of code with the PCA option until I found the single line that had been my bane:
![](chord.png)
All of my values were smaller than 0.5, and yet the threshold assumed for the chord diagram was 0.5, which was why the matrix that resulted was always filled with only 0s. Changing the threshold to 0 resulted in the following interactive diagram, with SA2 in red and SA1 in blue:
![](chorddiagram.png)
(The diagram can also be viewed by downloading the following folder, navigating to the folder in the terminal, and creating your own python http server by typing `python3 -m http.server 8000`)
As well as a graph of the reduced values:
![](graph.png)
After using the same troubleshooting method for LDA, I realized that the function visualizeFeaturesFolder had a third positional argument, priorKnowledge, which the function used to define the classes. This was at odds with the official guide itself, which instead stated that with LDA, a supervised model, "the required labels are taken from the subcategories of the input files (if available). These are provided through the respective filenames, using the string `---` as a separator." (See the rest of the guide on data visualization with pyAudioAnalysis [here](https://github.com/tyiannak/pyAudioAnalysis/wiki/6.-Data-visualization)). PCA instead did what the guide stated LDA would do:
![](names.png) 

I also created a spectrogram of one speaker from DR4 saying both SA1 and SA2 in the terminal using `python audioAnalysis.py fileSpectrogram -i FILENAME`.
SA1:
![](SA1_Spectrogram.png)
SA2:
![](SA2_Spetrogram.png)

Thus it seems that, overall, SA1 and SA2, when spoken by someone from the South Midland dialect region (see diagram below for map of area, with orange being South Midland), are quite similar.
![](https://upload.wikimedia.org/wikipedia/commons/9/99/Midland_American_English_map.jpg)