---
layout: post
title: Final_Project
---
Date: 01/07/2020
Today I began to seriously search for a large dataset to analyze. While I struggled to find the correct keywords for the search at first, I soon managed to find a dataset that perfectly fit my needs and was even compiled with my specific purpose in mind: https://www.kaggle.com/mfekadu/darpa-timit-acousticphonetic-continuous-speech. The ReadMe was extremely intriguing to read, as it joined both linguistics of American English dialects and discussed the several test/train splits the authors suggested. The dataset also contains a huge amount of data, which will afford me the opportunity to showcase my ability to clean and synthesize datasets. I also found a paper describing the analysis of audio using python, though I do not know how useful that will be until I dig into the dataset and its contents tomorrow.

Date: 01/08/2020
Today I began by reading in the dataset and exploring it to get my bearings as to how the researchers organized their information. I quickly realized that the dataset was mostly just for organization, with all of the audio data stored in the folders downloaded alongside the dataset. The audio data was in the waveform audio format, making it clear that I'd need to use pyAudioAnalysis, which I'd discovered yesterday. As a result, I read the general information offered by the developers before delving into feature extraction, which I will need to be able to classify the recordings. The sheer number of potential features overwhelmed me almost immediately, and so I took a step back, deciding instead to specifically research audio analysis for machine learning. Keeping in mind the massive amount of audio data I am working with, I decided to use Mel Frequency Cepstral Coefficients (MFCCs) as my feature of focus, as they, much like SVD, contain the most essential information from the audio. Tomorrow I plan to work on analyzing only one recording with pyAudioAnalysis, so that I can feel confident in my coding before applying it to the rest of the training set. I will also continue to consider which models to utilize in relating the audio with the text, as well as what I wish to do with this model beyond simply creating a good regression for it (I'm hoping to incorporate some Natural Language Processing).

Date: 01/09/2020
Today was a much more difficult and trying day. Installing pyAudioAnalysis was much more confusing than I expected, as it required a more intimate knowledge of my computer terminal than I possessed, and Lauren and I ran into multiple errors along the way. When I though I had finally finished, and attempted to import ShortTermFeatures from pyAudioAnalysis in my jupyter notebook, I received another error, this time stating that the package couldn't be found. After trying several possible solutions, Lauren and I found that I had a pyAudioAnalysis folder in two separate locations: under my machine learning folder and my anaconda folder. The former had the necessary files while the latter did not, and so we copied all the contents of the former into the latter, which finally resolved the error. In turn, however, a warning popped up, explaining that ffmpeg or avconv could not be found. Nevertheless, when I tested the functions of both audioBasicIO and ShortTermFeatures, everything worked fine, so hopefully the issue the warning discusses will not be a problem. Tomorrow I am planning to set up a function and a for loop to set up a new data frame with the MFCCs from all the recordings using ShortTermFeatures, as well as read up on the commands I am using from pyAudioAnalysis to figure out what each line I code actually does to the raw data of the recording. This way, I will be ready to train my model, which I now think will probably be KNN or a Support Vector Machine, as both are often mentioned in python audio analysis and even in pyAudioAnalysis examples themselves.

Date: 01/11/2020
Yesterday I began working on a function called audio_analysis that, given a recording's name (audio_file) and the name of the file containing the transcript of the recording (text_file), would append to a list, mfcc, the MFCCs of the recording and to another list, sentences, the transcript. While I originally struggled with how to append the sentences, as each had some integers at the start that were useless for my purpose, I soon realized that it was unnecessary to add each transcript to the sentences list, as each sentence has its own unique string associated with it. As a result, I decided to open the PROMPTS.TXT file, which contains a list of all the prompts, so as to compile a list that would show me the string for each sentence. Over the weekend, however, as I looked over my code, I remembered that the file names of the recordings already contain the string for the sentence spoken; thus, all I have to do is pull the string from the input audio_file without the .WAV.wav ending, an act which will be easily accomplished with .split(). In class yesterday I also read further on the many functions pyAudioAnalysis offers, including their classification and regression functions, the former of which uses KNN and SVM. Though I was tempted to use these, as they automatically perform much of the calculations and decisions for you, I decided that it would be better for me to set up and code the model on my own, as I want much more control over my data analysis. I am still struggling to choose a classification model to use, as, though KNN is typically used for audio classification, I do not know how successful it will be with so many variables. I am also curious to see how an unsupervised learning algorithm, like k-means clustering (which I at first confused with KNN while thinking about this issue), would do. I plan to work on polishing my audio_analysis function over the rest of the weekend and making significant progress on setting up a for loop to go through all the files in the train set, so as to be ready to try out KNN on the dataset on Monday.

Date: 01/13/2020
Today I focused on creating my training and testing dataframes, with the goal of running GridSearchCV with a KNN model. This plan seemed simple enough until I decided to change up my code from the weekend to include an additional for loop that would differentiate between training and testing data so I would not have to copy the code for the training data and customize it for use with the testing data. When I did so, I knew that the code would take a long time to run, as I have a massive amount of data, but I believed that I had no errors and so I would move easily on to the next step. However, I kept finding errors in the structure of the data and re-running the code only to find more errors, which soon gobbled up a lot of my time, though I used it to comment explanations on most lines of my code. I also had to rename the columns, which was also time consuming in its typing, and wait for GridSearchCV, looking for n_neighbors in the range of 1 to 20, to run. I was extremely disappointed to find that the highest R^2 value, with 19 nearest neighbors, was only 0.176, or 17.619%. Though I had not had very high expectations, this  value was still completely unexpected. As a result, I decided to narrow my data pool to only two sentences: SA1 and SA2, which were the most common of the sentences spoken. Doing so resulted in an R^2 value of 0.898, or 89.881%, a massive improvement over the model using the larger dataset. Consequently, I decided, especially with the limited amount of work time left, to focus my next model, k_means clustering, on the dataset with solely SA1 and SA2, which will also allow me to work with the results with much less trouble, as there'll be less data with which to wrestle. I'm interested to see which dialects, if any, give the computer more trouble.

Date: 01/15/2020
Today I worked on creating a dataframe with enough sentences that it would be more interesting to analyze than just a simple two-sentence classification without dropping below an R^2 value of 0.8. To do this, I ran through the dictionary of sentence: number of speakers that I had already made and created a list of tuples with (value, key) so that I could sort by descending order of frequency. From there, I appended just the sentence codes to a new list and then iterated through the list, adding a new sentence every time while checking to see if the R^2 value dropped under 0.8. To simplify finding the R^2 value, I created a function called analyze that takes in a training set, tr, and a testing set, te, and performs all the necessary dropping, scaling, and modeling before returning r2. I once again used GridSearchCV between 1 and 20 neighbors, even though I knew that would take a lot of time, as I would have to test over 1000 possibilities, with the hopes that, with some patience, I would not have to sacrifice the accuracy and precision of my model. If it takes over half an hour to run, however, I will have to limit the GridSearch to maybe only 10 to 20 neighbors, or even maybe 15 to 20 neighbors, as these are the ranges in which the neighbors for my other models can be found. I'm hoping to finish up my coding tonight so as to free up time during the review period to work on my final blogpost, my code to make it readable, and my presentation. I plan to also analyze the most confused sentences in the hopes of identifying the reason behind the confusion, which could be due to similarities in the text or difficulty with the accent.