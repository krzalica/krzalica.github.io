---
layout: post
title: Final_Project
---
Date: 01/07/2020
Today I began to seriously search for a large dataset to analyze. While I struggled to find the correct keywords for the search at first, I soon managed to find a dataset that perfectly fit my needs and was even compiled with my specific purpose in mind: https://www.kaggle.com/mfekadu/darpa-timit-acousticphonetic-continuous-speech. The ReadMe was extremely intriguing to read, as it joined both linguistics of American English dialects and discussed the several test/train splits the authors suggested. The dataset also contains a huge amount of data, which will afford me the opportunity to showcase my ability to clean and synthesize datasets. I also found a paper describing the analysis of audio using python, though I do not know how useful that will be until I dig into the dataset and its contents tomorrow.

Date: 01/08/2020
Today I began by reading in the dataset and exploring it to get my bearings as to how the researchers organized their information. I quickly realized that the dataset was mostly just for organization, with all of the audio data stored in the folders downloaded alongside the dataset. The audio data was in the waveform audio format, making it clear that I'd need to use pyAudioAnalysis, which I'd discovered yesterday. As a result, I read the general information offered by the developers before delving into feature extraction, which I will need to be able to classify the recordings. The sheer number of potential features overwhelmed me almost immediately, and so I took a step back, deciding instead to specifically research audio analysis for machine learning. Keeping in mind the massive amount of audio data I am working with, I decided to use Mel Frequency Cepstral Coefficients (MFCCs) as my feature of focus, as they, much like SVD, contain the most essential information from the audio. Tomorrow I plan to work on analyzing only one recording with pyAudioAnalysis, so that I can feel confident in my coding before applying it to the rest of the training set. I will also continue to consider which models to utilize in relating the audio with the text, as well as what I wish to do with this model beyond simply creating a good regression for it (I'm hoping to incorporate some Natural Language Processing).

Date: 01/09/2020
Today was a much more difficult and trying day. Installing pyAudioAnalysis was much more confusing than I expected, as it required a more intimate knowledge of my computer terminal than I possessed, and Lauren and I ran into multiple errors along the way. When I though I had finally finished, and attempted to import ShortTermFeatures from pyAudioAnalysis in my jupyter notebook, I received another error, this time stating that the package couldn't be found. After trying several possible solutions, Lauren and I found that I had a pyAudioAnalysis folder in two separate locations: under my machine learning folder and my anaconda folder. The former had the necessary files while the latter did not, and so we copied all the contents of the former into the latter, which finally resolved the error. In turn, however, a warning popped up, explaining that ffmpeg or avconv could not be found. Nevertheless, when I tested the functions of both audioBasicIO and ShortTermFeatures, everything worked fine, so hopefully the issue the warning discusses will not be a problem. Tomorrow I am planning to set up a function and a for loop to set up a new data frame with the MFCCs from all the recordings using ShortTermFeatures, as well as read up on the commands I am using from pyAudioAnalysis to figure out what each line I code actually does to the raw data of the recording. This way, I will be ready to train my model, which I now think will probably be KNN or a Support Vector Machine, as both are often mentioned in python audio analysis and even in pyAudioAnalysis examples themselves.

Date: 01/11/2019
Yesterday I began working on a function called audio_analysis that, given a recording's name (audio_file) and the name of the file containing the transcript of the recording (text_file), would append to a list, mfcc, the MFCCs of the recording and to another list, sentences, the transcript. While I originally struggled with how to append the sentences, as each had some integers at the start that were useless for my purpose, I soon realized that it was unnecessary to add each transcript to the sentences list, as each sentence has its own unique string associated with it. As a result, I decided to open the PROMPTS.TXT file, which contains a list of all the prompts, so as to compile a list that would show me the string for each sentence. Over the weekend, however, as I looked over my code, I remembered that the file names of the recordings already contain the string for the sentence spoken; thus, all I have to do is pull the string from the input audio_file without the .WAV.wav ending, an act which will be easily accomplished with .split(). In class yesterday I also read further on the many functions pyAudioAnalysis offers, including their classification and regression functions, the former of which uses KNN and SVM. Though I was tempted to use these, as they automatically perform much of the calculations and decisions for you, I decided that it would be better for me to set up and code the model on my own, as I want much more control over my data analysis. I am still struggling to choose a classification model to use, as, though KNN is typically used for audio classification, I do not know how successful it will be with so many variables. I am also curious to see how an unsupervised learning algorithm, like k-means clustering (which I at first confused with KNN while thinking about this issue), would do. I plan to work on polishing my audio_analysis function over the rest of the weekend and making significant progress on setting up a for loop to go through all the files in the train set, so as to be ready to try out KNN on the dataset on Monday.