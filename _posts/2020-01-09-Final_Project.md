---
layout: post
title: Final_Project
---
Date: 01/07/2020
Today I began to seriously search for a large dataset to analyze. While I struggled to find the correct keywords for the search at first, I soon managed to find a dataset that perfectly fit my needs and was even compiled with my specific purpose in mind: https://www.kaggle.com/mfekadu/darpa-timit-acousticphonetic-continuous-speech. The ReadMe was extremely intriguing to read, as it joined both linguistics of American English dialects and discussed the several test/train splits the authors suggested. The dataset also contains a huge amount of data, which will afford me the opportunity to showcase my ability to clean and synthesize datasets. I also found a paper describing the analysis of audio using python, though I do not know how useful that will be until I dig into the dataset and its contents tomorrow.

Date: 01/08/2020
Today I began by reading in the dataset and exploring it to get my bearings as to how the researchers organized their information. I quickly realized that the dataset was mostly just for organization, with all of the audio data stored in the folders downloaded alongside the dataset. The audio data was in the waveform audio format, making it clear that I'd need to use pyAudioAnalysis, which I'd discovered yesterday. As a result, I read the general information offered by the developers before delving into feature extraction, which I will need to be able to classify the recordings. The sheer number of potential features overwhelmed me almost immediately, and so I took a step back, deciding instead to specifically research audio analysis for machine learning. Keeping in mind the massive amount of audio data I am working with, I decided to use Mel Frequency Cepstral Coefficients (MFCCs) as my feature of focus, as they, much like SVD, contain the most essential information from the audio. Tomorrow I plan to work on analyzing only one recording with pyAudioAnalysis, so that I can feel confident in my coding before applying it to the rest of the training set. I will also continue to consider which models to utilize in relating the audio with the text, as well as what I wish to do with this model beyond simply creating a good regression for it (I'm hoping to incorporate some Natural Language Processing).

Date: 01/09/2020
Today was a much more difficult and trying day. Installing pyAudioAnalysis was much more confusing than I expected, as it required a more intimate knowledge of my computer terminal than I possessed, and Lauren and I ran into multiple errors along the way. When I though I had finally finished, and attempted to import ShortTermFeatures from pyAudioAnalysis in my jupyter notebook, I received another error, this time stating that the package couldn't be found. After trying several possible solutions, Lauren and I found that I had a pyAudioAnalysis folder in two separate locations: under my machine learning folder and my anaconda folder. The former had the necessary files while the latter did not, and so we copied all the contents of the former into the latter, which finally resolved the error. In turn, however, a warning popped up, explaining that ffmpeg or avconv could not be found. Nevertheless, when I tested the functions of both audioBasicIO and ShortTermFeatures, everything worked fine, so hopefully the issue the warning discusses will not be a problem. Tomorrow I am planning to set up a function and a for loop to set up a new data frame with the MFCCs from all the recordings using ShortTermFeatures, as well as read up on the commands I am using from pyAudioAnalysis to figure out what each line I code actually does to the raw data of the recording. This way, I will be ready to train my model, which I now think will probably be KNN or a Support Vector Machine, as both are often mentioned in python audio analysis and even in pyAudioAnalysis examples themselves.